{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e717ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script extracts text from PDF files in a specified directory and saves the text to new files.\n",
    "# It uses the pdfplumber library to read the PDFs and extract text from each page.\n",
    "\n",
    "import pdfplumber, os\n",
    "\n",
    "pdf_dir = '../data/raw_pdfs/'\n",
    "out_dir = '../data/extracted_texts/'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for fname in os.listdir(pdf_dir):\n",
    "    if fname.endswith('.pdf'):\n",
    "        with pdfplumber.open(os.path.join(pdf_dir, fname)) as pdf:\n",
    "            text = ''\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + '\\n'\n",
    "        out_path = os.path.join(out_dir, fname.replace('.pdf', '.txt'))\n",
    "        with open(out_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5149643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the extracted text\n",
    "# This part cleans up the text by removing extra whitespace and newlines.\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "# Example usage\n",
    "for fname in os.listdir(out_dir):\n",
    "    with open(os.path.join(out_dir, fname), 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "    cleaned = clean_text(raw_text)\n",
    "    with open(os.path.join(out_dir, fname.replace('.txt', '.cleaned.txt')), 'w', encoding='utf-8') as f:\n",
    "        f.write(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4106a73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: ../data/subdomains/company_law\\01. Companies Act No. 7 of 2007.cleaned.txt\n",
      "Processed: ../data/subdomains/tax_law\\02. Inland Revenue Act_No_24_2017_E.cleaned.txt\n",
      "Processed: ../data/subdomains/tax_law\\03. Inland Revenue (Amendment) Act No. 2 of 2025.cleaned.txt\n",
      "Processed: ../data/subdomains/banking_law\\04. Banking Act 30_1988.cleaned.txt\n",
      "Processed: ../data/subdomains/banking_law\\05. Banking_Amendment_Act_No_24_of_2024_e.cleaned.txt\n",
      "Processed: ../data/subdomains/banking_law\\06. Banking (Special Provisions) Act, No. 17 of 2023.cleaned.txt\n",
      "Processed: ../data/subdomains/securities_law\\07. Securities and Exchange Commission of Sri Lanka.cleaned.txt\n",
      "Processed: ../data/subdomains/insolvency_law\\08. INSOLVENTS [Cap.103 - Lanka Law.cleaned.txt\n",
      "Processed: ../data/subdomains/contract_law\\09. Sale_of_Goods_Ordinance_No_11_of_1896_of.cleaned.txt\n",
      "Processed: ../data/subdomains/negotiable_instruments_law\\10. Bills of Exchanger Ordinance.cleaned.txt\n",
      "Processed: ../data/subdomains/negotiable_instruments_law\\11. BILLS OF EXCHANGE (AMENDMENT).cleaned.txt\n",
      "Processed: ../data/subdomains/consumer_law\\12. Consumer Affairs Authority Act No 9 of 2003.cleaned.txt\n",
      "Processed: ../data/subdomains/ip_law\\13. Intellectual_Property_Act_No_36_of_2003.cleaned.txt\n",
      "Processed: ../data/subdomains/arbitration_law\\14. ARBITRATION-ACT No 11 of 1995.cleaned.txt\n",
      "Processed: ../data/subdomains/trust_law\\16. Trust Ordinance.cleaned.txt\n",
      "Processed: ../data/subdomains/electronic_transactions_law\\17. ElectronicTransactionActNo19of2006.cleaned.txt\n",
      "Processed: ../data/subdomains/foreign_exchange_law\\18. Foreign Exchange ACT NO 12 of 2017.cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "# This script processes legal documents in various subdomains using spaCy for NLP tasks.\n",
    "# It tokenizes the text, segments sentences, and performs named entity recognition (NER).\n",
    "\n",
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# List of subdomain folders\n",
    "subdomains = [\n",
    "    \"company_law\", \"tax_law\", \"banking_law\", \"securities_law\", \"insolvency_law\",\n",
    "    \"contract_law\", \"negotiable_instruments_law\", \"consumer_law\", \"ip_law\",\n",
    "    \"arbitration_law\", \"trust_law\", \"electronic_transactions_law\", \"foreign_exchange_law\"\n",
    "]\n",
    "\n",
    "base_dir = \"../data/subdomains/\"\n",
    "\n",
    "for subdomain in subdomains:\n",
    "    folder = os.path.join(base_dir, subdomain)\n",
    "    for fname in os.listdir(folder):\n",
    "        if fname.endswith(\".cleaned.txt\"):\n",
    "            file_path = os.path.join(folder, fname)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "\n",
    "            # Run NLP pipeline\n",
    "            doc = nlp(text)\n",
    "\n",
    "            # Tokenization (word tokens, excluding stopwords and punctuation)\n",
    "            tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "            # Sentence segmentation\n",
    "            sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "            # Named Entity Recognition\n",
    "            entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "            # Save tokens\n",
    "            token_out = file_path.replace(\".cleaned.txt\", \".tokens.txt\")\n",
    "            with open(token_out, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\" \".join(tokens))\n",
    "\n",
    "            # Save sentences\n",
    "            sent_out = file_path.replace(\".cleaned.txt\", \".sentences.txt\")\n",
    "            with open(sent_out, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\\n\".join(sentences))\n",
    "\n",
    "            # Save entities as CSV\n",
    "            ent_out = file_path.replace(\".cleaned.txt\", \".entities.csv\")\n",
    "            df = pd.DataFrame(entities, columns=[\"entity\", \"label\"])\n",
    "            df.to_csv(ent_out, index=False)\n",
    "\n",
    "            print(f\"Processed: {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
